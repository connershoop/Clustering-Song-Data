ggplot(x, aes(time, sentimentTot), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("#soc119 tweets per every 10 minutes") +
ggtitle("freq of tweets containing #Soc119 Jan22 to Jan 29, 2018")
#wordcloud
counter <- doc_term %>%
count(word) %>%
arrange(desc(n))
# find sentiment per 10 minutes in a week
time <- "2018-01-22 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
for (i in 1:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)+x[i-1,2]
}
#plot function for that time
ggplot(x, aes(time, sentimentTot), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("#soc119 tweets per every 10 minutes") +
ggtitle("freq of tweets containing #Soc119 Jan22 to Jan 29, 2018")
View(x)
for (i in 2:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)+x[i-1,2]
}
View(x)
i=1006
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)+x[i-1,2]
View(x)
array1*array2*doc_sentiment$score
sum(array1*array2*doc_sentiment$score)
array1 <- doc_sentiment$time > time
# find sentiment per 10 minutes in a week
time <- "2018-01-22 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
array1 <- doc_sentiment$time > time
array2 <- doc_sentiment$time < time+60*10
x[1,2] <- sum(array1*array2*doc_sentiment$score)
for (i in 2:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)+x[i-1,2]
}
View(x)
#plot function for that time
ggplot(x, aes(time, sentimentTot), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("#soc119 tweets per every 10 minutes") +
ggtitle("freq of tweets containing #Soc119 Jan22 to Jan 29, 2018")
#plot function for that time
ggplot(x, aes(time, sentimentTot), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("Sentiment per 10 minutes intervals (-5 to 5 scale per word)") +
ggtitle("Sentiment of tweets containing containing #Soc119 Jan22 to Jan 29, 2018")
####### ####### ####### ####### ####### ####### ####### #######
####### find sentiment per 10 minutes in a week
time <- "2018-01-22 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
for (i in 1:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)
}
#plot function for that time
ggplot(x, aes(time, sentimentTot), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("Sentiment per 10 minutes intervals (-5 to 5 scale per word)") +
ggtitle("Sentiment of tweets containing containing #Soc119 Jan22 to Jan 29, 2018")
####### ####### ####### ####### ####### ####### ####### #######
####### find cumulativew sentiment per 10 minutes in a week
time <- "2018-01-22 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
array1 <- doc_sentiment$time > time
array2 <- doc_sentiment$time < time+60*10
x[1,2] <- sum(array1*array2*doc_sentiment$score)
for (i in 2:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)+x[i-1,2]
}
#plot function for that time
ggplot(x, aes(time, sentimentTot), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("sentiment (-5 to 5 scale per word)") +
ggtitle("Cumulative sentiment of tweets containing containing #Soc119 Jan22 to Jan 29, 2018")
####### ####### ####### ####### ####### ####### ####### #######
####### find sentiment per 10 minutes in a week
time <- "2018-01-22 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
for (i in 1:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)
}
#plot function for that time
ggplot(x, aes(time, sentimentTot), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("Sentiment per 10 minutes intervals (-5 to 5 scale per word)") +
ggtitle("Sentiment of tweets containing containing #Soc119 Jan22 to Jan 29, 2018")
View(doc_term)
View(doc_article)
doc_article %>%
group_by(document)
View(doc_article)
doc_article <- doc_article %>%
group_by(document)
View(doc_article)
doc_term_all <- doc_article %>%
unnest_tokens(word, article)
View(doc_term_all)
View(doc_term_all)
doc_term_all <- doc_article %>%
unnest_tokens(word, article) %>%
group_by(document) %>%
count(word)
View(doc_term_all)
doc_term_all <- doc_article %>%
unnest_tokens(word, article) %>%
group_by(document) %>%
mutate(n,sum(word))
doc_term_all <- doc_article %>%
unnest_tokens(word, article) %>%
group_by(document) %>%
mutate("n",sum(word))
?mutate
doc_term_all <- doc_article %>%
unnest_tokens(word, article) %>%
group_by(document) %>%
mutate(n , sum(word))
doc_term_all <- doc_article %>%
unnest_tokens(word, article) %>%
group_by(document) %>%
mutate(n , sum(word))
doc_term_all <- doc_article %>%
unnest_tokens(word, article) %>%
group_by(document) %>%
summarise(total=n()) %>%
arrange(desc(total))
View(doc_term_all)
View(doc_article)
View(doc_article)
View(doc_term_all)
dot_term_all[1:10]
doc_term_all[1:10]
doc_term_all[,1:10]
doc_term_all[1,1:10]
doc_term_all[1,10]
doc_term_all[2,10]
doc_term_all[1:10,1]
nrow[doc_term_all]
nrow(doc_term_all)
bottom10 <- doc_term_all(nrow(doc_term_all):nrow(doc_term_all)-10)
bottom10 <- doc_term_all(nrow(doc_term_all):nrow(doc_term_all)-10,1)
bottom10 <- doc_term_all[nrow(doc_term_all):nrow(doc_term_all)-10,1]
View(bottom10)
bottom10 <- doc_term_all[nrow(doc_term_all):(nrow(doc_term_all)-10),1]
View(bottom10)
top10 <- doc_term_all[1:10,1]
bottom10 <- doc_term_all[nrow(doc_term_all):(nrow(doc_term_all)-9),1]
View(bottom10)
top10 <- doc_term_all[1:10,1]
bottom10 <- doc_term_all[nrow(doc_term_all):(nrow(doc_term_all)-9),1]
top10tweets <- inner_join(top10,doc_article)
bottom10tweets <- inner_join(bottom10,doc_article)
View(bottom10tweets)
View(top10tweets)
write_excel_csv(top10tweets)
setwd("~/Documents/Twitter/Soc119Tweets")
write_excel_csv(top10tweets)
write_excel_csv(top10tweets, path = getwd())
write_excel_csv(top10tweets, path = top10Jan22toJan29.csv)
write_excel_csv(top10tweets, path = "top10Jan22toJan29.csv")
write_excel_csv(bottom10tweets, path = "bottom10Jan22toJan29.csv")
####### ####### ####### ####### ####### ####### ####### #######
####### find cumulativew sentiment per 10 minutes in a week
time <- "2018-01-22 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
array1 <- doc_sentiment$time > time
array2 <- doc_sentiment$time < time+60*10
x[1,2] <- sum(array1*array2*doc_sentiment$score)
for (i in 2:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)+x[i-1,2]
}
#plot function for that time
ggplot(x, aes(time, sentimentTot), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("sentiment (-5 to 5 scale per word)") +
ggtitle("Cumulative sentiment of tweets containing containing #Soc119 Jan22 to Jan 29, 2018")
#plot function for that time
ggplot(x, aes(time, sentimentTot), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("sentiment (-5 negative to 5 positive scale per word)") +
ggtitle("Cumulative sentiment of tweets containing containing #Soc119 Jan22 to Jan 29, 2018")
#plot function for that time
ggplot(x, aes(time, sentimentTot), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("sentiment (-5 negative to 5 positive scale per word)") +
ggtitle("Cumulative sentiment of tweets containing containing #Soc119 Jan 22 to Jan 29, 2018")
####### ####### ####### ####### ####### ####### ####### #######
####### find cumulativew sentiment per 10 minutes in a week
time <- "2018-01-22 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
array1 <- doc_sentiment$time > time
array2 <- doc_sentiment$time < time+60*10
x[1,2] <- sum(array1*array2*doc_sentiment$score)
for (i in 2:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)+x[i-1,2]
}
####### ####### ####### ####### ####### ####### ####### #######
####### find sentiment per 10 minutes in a week
time <- "2018-01-22 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
for (i in 1:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)
}
#plot function for that time
ggplot(x, aes(time, sentimentTot), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("Sentiment per 10 minutes intervals (-5 to 5 scale per word)") +
ggtitle("Sentiment of tweets containing containing #Soc119 Jan22 to Jan 29, 2018")
#plot function for that time
ggplot(x, aes(time, sentimentTot), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("Sentiment per 10 minutes intervals (-5 negative to 5 positive scale per word)") +
ggtitle("Sentiment of tweets containing containing #Soc119 Jan22 to Jan 29, 2018")
#plot function for that time
ggplot(x, aes(time, sentimentTot), expand(0,0)) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("Sentiment per 10 minutes intervals (-5 negative to 5 positive scale per word)") +
ggtitle("Sentiment of tweets containing containing #Soc119 Jan22 to Jan 29, 2018")
#plot function for that time
ggplot(x, aes(time, sentimentTot)) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("Sentiment per 10 minutes intervals (-5 negative to 5 positive scale per word)") +
ggtitle("Sentiment of tweets containing containing #Soc119 Jan22 to Jan 29, 2018")
####### ####### ####### ####### ####### ####### ####### #######
####### find cumulativew sentiment per 10 minutes in a week
time <- "2018-01-22 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
array1 <- doc_sentiment$time > time
array2 <- doc_sentiment$time < time+60*10
x[1,2] <- sum(array1*array2*doc_sentiment$score)
for (i in 2:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)+x[i-1,2]
}
#plot function for that time
ggplot(x, aes(time, sentimentTot), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("sentiment (-5 negative to 5 positive scale per word)") +
ggtitle("Cumulative sentiment of tweets containing containing #Soc119 Jan 22 to Jan 29, 2018")
View(doc_article)
source('~/Documents/RStudio/Soc119/TwitterPractice.R', echo=TRUE)
#setup with authentication key for twitter API
consumer_key <- 'eZPpCGjwmbZTV8hH0BIAk74pv'
consumer_secret <- 'PLRTEqmDo2Tog32uvpUFL72P64TNR2BLfKepgefcQLJlI2jgig'
access_token <- '915228318173102081-3iAy2xUbmZVm8MREvXxqgp1o7WHF5h4'
access_secret <- 'bgqJKoZhUXOAso2od4PuzirrmZodOsGRhLj8geq8fMwA2'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#get current date and yesterday
#need to correct for difference in date for UTC still
currentDate  <- Sys.Date()
#attr(currentDate, "tzone") <- "UTC" #doesn't work
yesterday <- currentDate -1
#make any date
date <- as.Date("2018-01-29")
####### ####### ####### ####### ####### ####### ####### #######
#get tweets by keyword
myTweets <- searchTwitter('#soc119 -filter:retweets', since = toString(date),
until = toString(date+7) ,n=9999)
myTweets <- twListToDF(myTweets)
View(myTweets)
#make any date
date <- as.Date("2018-01-22")
####### ####### ####### ####### ####### ####### ####### #######
#get tweets by keyword
myTweets <- searchTwitter('#soc119 -filter:retweets', since = toString(date),
until = toString(date+6) ,n=9999)
myTweets <- twListToDF(myTweets)
#make any date
date <- as.Date("2018-01-29")
####### ####### ####### ####### ####### ####### ####### #######
#get tweets by keyword
myTweets <- searchTwitter('#soc119 -filter:retweets', since = toString(date),
until = toString(date+7) ,n=9999)
myTweets <- twListToDF(myTweets)
####### ####### ####### ####### ####### ####### ####### #######
####### BASIC FIRST MANIPULATION OF DATA FRAME
#reduce data frame and convert time to POSIXct object
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text, time=myTweets$created)
doc_article$time <- as.POSIXct(doc_article$time)
#make stop words
custom_stop_words <- c("https", "t","t.co", "soc119","it's", "people")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
#make lower case
doc_term$word <- tolower(doc_term$word)
####### ####### ####### ####### ####### ####### ####### #######
# find time per 10 minutes that week
# vs number of tweets in that 10 minute interval
time <- "2018-01-22 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","n")
x$time <- seq(time,time+1007*10*60,
10*60)
for (i in 1:nrow) {
array1 <- doc_article$time > time+(i-1)*60*10
array2 <- doc_article$time < time+i*60*10
x[i,2] <- sum(array1*array2)
}
#plot function for that time
ggplot(x, aes(time, n), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("#soc119 tweets per every 10 minutes") +
ggtitle("freq of tweets containing #Soc119 Jan22 to Jan 29, 2018")
####### ####### ####### ####### ####### ####### ####### #######
# find time per 10 minutes that week
# vs number of tweets in that 10 minute interval
time <- "2018-01-29 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","n")
x$time <- seq(time,time+1007*10*60,
10*60)
for (i in 1:nrow) {
array1 <- doc_article$time > time+(i-1)*60*10
array2 <- doc_article$time < time+i*60*10
x[i,2] <- sum(array1*array2)
}
#plot function for that time
ggplot(x, aes(time, n), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("#soc119 tweets per every 10 minutes") +
ggtitle("freq of tweets containing #Soc119 Jan22 to Jan 29, 2018")
####### ####### ####### ####### ####### ####### ####### #######
####### find sentiment per 10 minutes in a week
time <- "2018-01-22 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
for (i in 1:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)
}
#plot function for that time
ggplot(x, aes(time, sentimentTot)) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("Sentiment per 10 minutes intervals (-5 negative to 5 positive scale per word)") +
ggtitle("Sentiment of tweets containing containing #Soc119 Jan22 to Jan 29, 2018")
#get doc_sentiment
doc_sentiment <- doc_term %>%
inner_join(afinn)
afinn <- get_sentiments('afinn')
#get doc_sentiment
doc_sentiment <- doc_term %>%
inner_join(afinn)
####### ####### ####### ####### ####### ####### ####### #######
####### find sentiment per 10 minutes in a week
time <- "2018-01-29 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
for (i in 1:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)
}
#plot function for that time
ggplot(x, aes(time, sentimentTot)) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("Sentiment per 10 minutes intervals (-5 negative to 5 positive scale per word)") +
ggtitle("Sentiment of tweets containing containing #Soc119 Jan22 to Jan 29, 2018")
####### ####### ####### ####### ####### ####### ####### #######
####### find cumulativew sentiment per 10 minutes in a week
time <- "2018-01-22 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
array1 <- doc_sentiment$time > time
array2 <- doc_sentiment$time < time+60*10
x[1,2] <- sum(array1*array2*doc_sentiment$score)
for (i in 2:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)+x[i-1,2]
}
#plot function for that time
ggplot(x, aes(time, sentimentTot)) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("sentiment (-5 negative to 5 positive scale per word)") +
ggtitle("Cumulative sentiment of tweets containing containing #Soc119 Jan 22 to Jan 29, 2018")
####### ####### ####### ####### ####### ####### ####### #######
####### find cumulativew sentiment per 10 minutes in a week
time <- "2018-01-29 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
array1 <- doc_sentiment$time > time
array2 <- doc_sentiment$time < time+60*10
x[1,2] <- sum(array1*array2*doc_sentiment$score)
for (i in 2:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)+x[i-1,2]
}
#plot function for that time
ggplot(x, aes(time, sentimentTot)) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("sentiment (-5 negative to 5 positive scale per word)") +
ggtitle("Cumulative sentiment of tweets containing containing #Soc119 Jan 22 to Jan 29, 2018")
####### ####### ####### ####### ####### ####### ####### #######
#wordcloud
counter <- doc_term %>%
count(word) %>%
arrange(desc(n))
wordcloud(counter$word, counter$n, max.words = 100)
wordcloud(counter$word, counter$n, max.words = 100)
View(afinn)
####### ####### ####### ####### ####### ####### ####### #######
####### GETTING RID OF SYMBOLS AND NUMBERS
#unnest @ and other symbols
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
doc_term <- doc_term %>%
unnest_tokens(word, word, token = "regex", pattern = unnest_reg)
#unnest terms with 0-9 in them
toMatch <- c("0","1", "2", "3","4", "5", "6","7", "8", "9")
matches <- as.data.frame(grep(paste(toMatch,collapse="|"),
doc_term$word, value=TRUE))
colnames(matches) <- c("word")
doc_term <- doc_term %>%
anti_join(matches)
View(doc_term)
#stemWords
doc_term <- doc_term %>%
mutate(word = wordStem(word))
####### ####### ####### ####### ####### ####### ####### #######
#wordcloud
counter <- doc_term %>%
count(word) %>%
arrange(desc(n))
wordcloud(counter$word, counter$n, max.words = 100)
####### ####### ####### ####### ####### ####### ####### #######
#wordcloud
counter <- doc_term %>%
count(word) %>%
arrange(desc(n))
wordcloud(counter$word, counter$n, max.words = 100)
?wordStem
library(tm)
install.packages("tm")
library(tm)
stemCompletion(doc_term)
?stemCompletion
stemCompletion(doc_term, dictionary = crude)
setwd("~/Documents/GitHub/Clustering-Song-Data")
what
